{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.utils import get_transform, collate_fn, Custom_Dataset, Evaluator\n",
    "from src.plotting import show_prediction\n",
    "from src.saving import Model_Manager\n",
    "from src.deployable_network import Deployable_Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Deployable Network\n",
    "\n",
    "To test how the network could be deployed in real time on a moving belt, some experimentation was done with a Deployable_Network. In this section of the notebook, bounding box ensembling techniques may be tested. These include non-maximum supression and weighted bounding box fusion. Provide the location and model filename to (`file_location` and `model_filename`) to load the trained object detector - if no `version` is assigned, the latest will be loaded.\n",
    "\n",
    "Class prioritisation should be implimented in the case where the two classes are similar in edge cases, leading to the model making ambiguous predictions, for instance where multiple predictions are made for one object. `similar_classes` holds a list of tuples , each of which corresponding to a given prioritisation. For instance, to prioritise class 1 over class 3, add the tuple `(1,3)` to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detector + classifier location\n",
    "file_location = 'path/to/models'\n",
    "model_filename = 'Model Name'\n",
    "path = os.path.join(file_location, model_filename)\n",
    "\n",
    "#Load the model and (if requested) any annotations\n",
    "model_manager = Model_Manager(path)\n",
    "give_annotations = True\n",
    "version = 1\n",
    "if give_annotations:\n",
    "    object_detector, annotations_paths = model_manager.load(version=version, give_annotations=give_annotations)\n",
    "else:\n",
    "    object_detector = model_manager.load(version=version, give_annotations=give_annotations)\n",
    "\n",
    "#Similar classes for fine classifier\n",
    "similar_classes = [(1,3), (2,3)]     #Class 1 is prioritised over class 3 and class 2 is prioritised over class 3\n",
    "\n",
    "#Initialise fine detector\n",
    "model = Deployable_Network(object_detector, similar_classes)\n",
    "\n",
    "#Send model to device\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the precision of this deployable model, it is helpful to test on images it has not trained on. If train/test subsets were saved during training, `give_annotations` should be set to true in the cell above and the train/test annotations will be loaded. These contain image paths and annotation data for the ground truths (boxes, scores and labels), along with the categories used. The cell bellow produces a dataloader using only the test dataset, so the deployable model may be evaluated on images its base object detector has not yet seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download dataset\n",
    "#annotations_path = '/Users/benmarrett/Documents/EMR/Model Implimentation/Faster R-CNN/formatted_annotations.json'\n",
    "dataset = Custom_Dataset(annotations_paths[0], get_transform(train=False))\n",
    "\n",
    "train_percent = 1.     #Percentage of data for training\n",
    "final_train_ind = int(len(dataset) * train_percent)\n",
    "indices = torch.randperm(len(dataset)).tolist()        #Randomise train + test images\n",
    "\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:final_train_ind])   #Convert to subset so that evaluator works\n",
    "\n",
    "#Convert to dataloaders\n",
    "batch_size = 1\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell bellow simply compares the base object detector to the deployable model. Give the `path` to the target image and the cell shows predictions from both the base object detector and the full deployable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Category names for labelling image\n",
    "categories = {category['id'] : category['name'] for category in dataset.dataset.annotations['categories']}\n",
    "\n",
    "#Path to image\n",
    "path = 'path/to/image.jpg'\n",
    "\n",
    "#Show predictions from base object detector and deployable model\n",
    "show_prediction(object_detector, categories, path, figsize=(12,12))\n",
    "show_prediction(model, categories, path, figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the deployable_model and show confusion matrix\n",
    "evaluator = Evaluator(data_loader)\n",
    "evaluator.evaluate(model)\n",
    "evaluator.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Inference\n",
    "\n",
    "The was also a little experimentation with performing live inference on a moving belt. Begin by giving the deployable model information such as the camera field of view (`camera_fov`), the height of the camera above the belt (`camera_height`), the belt speed (`belt_speed`) and direction the belt is moving with respect to the image (`belt_direction`). Using this information, at each inference the deployable model translates any previous predictions across the image using the belt velocity, giving the predicted object's location where it would be at the next stage of inference. If the object is still within the frame, it may be predicted again at the current stage of inference, and so the deployable model will combine such predictions using weighted bounding box fusion. If any predictions out of the frame, their bounding box coordinates are outputted to be ejected from the feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fov(sensor_width, focal_length):\n",
    "    '''Calculates the camera field of view'''\n",
    "\n",
    "    return 2 * np.arctan(sensor_width / (2*focal_length))\n",
    "\n",
    "#Camera dimensions (meters)\n",
    "sensor_width = 5.88e-3\n",
    "focal_length = 4.25e-3\n",
    "camera_height = 1      #Height above picking belt\n",
    "\n",
    "#Calculate camera field of view\n",
    "camera_fov = calc_fov(sensor_width, focal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dimension = (1080,1920)  #Dimension of image from deployed camera. Must be consistent\n",
    "belt_speed = 0.6               #Belt speed given in meters per second\n",
    "belt_direction = 'up'          #Direction of moving belt\n",
    "\n",
    "#Initialise model parameters usinh \n",
    "model.init_live_inference(image_dimension, camera_fov, camera_height, belt_speed, belt_direction)\n",
    "\n",
    "run = True\n",
    "\n",
    "while run:\n",
    "    #Path to new image\n",
    "    new_path = 'path/to/new/image.jpg'\n",
    "\n",
    "    #Perform inference\n",
    "    show_prediction(model, categories, new_path, figsize=(12,12))\n",
    "\n",
    "    #Get bboxes for objects out of frame for ejection\n",
    "    eject_boxes = model.return_boxes\n",
    "\n",
    "    #Ask whether to quit live inference\n",
    "    user_input = input(\"Continue testing? [y/N]\")\n",
    "    if user_input == 'N':\n",
    "        print(\"Exiting loop\")\n",
    "        run = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
